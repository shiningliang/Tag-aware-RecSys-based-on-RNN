**本文旨在记录作为推荐系统技术基础的几种矩阵分解方法以及应用，主要包括以下内容：**
1. 常见的几种MF方式
    * 特征值与特征向量
    * SVD分解
    * LU分解
    * QR分解
2. 推荐系统中的MF
    * Basic MF
    * Regularized MF
    * PMF
    * SVD++
    * timeSVD++
    * SVD+CF
# 常见的几种矩阵分解方式
矩阵分解(decomposition,factorization)是将矩阵拆解为数个矩阵的乘积，可分为三角分解、满秩分解、QR分解、Jordan分解和SVD分解等，常见的有三种：1)三角分解，2)QR分解，3)SVD分解
## 特征值与特征向量
矩阵乘法真正的含义是变换,若以其中一个向量A为核心，则B的主要作用是使A发生如下变化：
1. 伸缩 #TODO
2. 切变 #TODO
3. 旋转 #TODO

书中对特征向量的定义：
> 设A是n阶方阵，如果存在 $\lambda$和n维非零向量$x$，使得$Ax=\lambda x$成立，则 $\lambda$成为方阵$A$的一个特征值，$x$为方阵$A$对应于或属于特征值 $\lambda$的一个特征向量。

因此，特征向量的代数含义是：将矩阵乘法转换为数乘操作；特征向量的几何含义是：特征向量通过方阵A变换只进行伸缩，而保持特征向量的方向不变。特征值为缩放倍数，而特征向量在几何上是一个点，从原点到该点的方向表示向量的方向。所以，**从旋转和缩放的角度，一个矩阵变换就是，旋转->沿坐标轴缩放->转回来，三步操作，表达为：**
$$
T=U\Sigma U^T
$$
**对于非正定矩阵，也可分解为，旋转->沿坐标轴缩放->旋转，三步，只不过最后一步和第一步的两个旋转不再是对称的关系，表达为：**
$$
T=U\Sigma V^T
$$
**这个就是SVD分解，参考自[知乎](https://www.zhihu.com/question/30094611)**
*特征向量有一个重要的性质：同一特征值的任意多个特征向量的线性组合仍然是A属于同一特征值的特征向量*
从线性空间的角度看，在一个定义了内积的线性空间里，对一个N阶对称方阵进行特征分解，就是产生了该空间的N个标准正交基，然后把矩阵投影到这N个基上。N个特征向量就是N个标准正交基，而特征值的模则代表矩阵在每个基上的投影长度。特征值越大，说明矩阵在对应的特征向量上的方差越大，功率越大，信息量越多。不过，特征值分解也有很多的局限，比如说变换的矩阵必须是方阵。
在机器学习特征提取中，意思就是最大特征值对应的特征向量方向上包含最多的信息量，如果某几个特征值很小，说明这几个方向信息量很小，可以用来降维，也就是删除小特征值对应方向的数据，只保留大特征值方向对应的数据，这样做以后数据量减小，但有用信息量变化不大，PCA降维就是基于这种思路。
## 奇异值(Single Value Decomposition，SVD)分解
特征值分解是一个提取矩阵特征很不错的方法，但是它只适用于方阵。而在现实的世界中，我们看到的大部分矩阵都不是方阵。奇异值分解是一个适用于任意矩阵的一种分解方法，对于特征值分解公式，$AA^T$是方阵，求$AA^T$的特征值，即 $(AA^T)x=\lambda x$，此时求得的特征值对应奇异值的平方，求得的特征向量$v$称为右奇异向量，另外还可以得到：
$$
\sigma_i=\sqrt{\lambda_i}
\\u_i=\frac{Av_i}{\sigma_i}
$$
所求的$u_i$就是左奇异向量，$\sigma_i$就是奇异值。

[奇异值分解的几何意义](http://blog.sciencenet.cn/blog-696950-699432.html)

[矩阵奇异值分解及其应用](http://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html)
## 三角分解（LU）分解
矩阵的LU分解是将一个矩阵分解为一个下三角矩阵与上三角矩阵的乘积。本质上，LU分解是高斯消元的一种表达方式。首先，对矩阵A通过初等行变换将其变为一个上三角矩阵，然后，将原始矩阵A变为上三角矩阵的过程，其对应的变换矩阵为一个下三角矩阵。这中间的过程，就是Doolittle algorithm(杜尔里特算法)。若$Ax=b$是一个非奇异系统，那么高斯消元法将A简化为一个上三角矩阵。若主轴上没有0值，则无需交换行，因此只需进行第3类初等变换(把第i行加上第j行的k倍)即可完成变换。例如
$$
\begin{pmatrix}
2 & 2 & 2\\
4 & 7 & 7\\
6 & 18 & 22
\end{pmatrix}
R_2-2R_1\rightarrow R_3-2R_1\rightarrow
\begin{pmatrix}
2 & 2 & 2\\
0 & 3 & 3\\
0 & 12 & 16
\end{pmatrix}
R_3-4R_2\rightarrow
\begin{pmatrix}
2 & 2 & 2\\
0 & 3 & 3\\
0 & 0 & 4
\end{pmatrix}=U
$$
第3类行变换可以通过左乘相应的初等矩阵image实现，对上例来说进行的3个变换就是相应初等矩阵的乘积。注意最右边是一个下三角矩阵L
$$
G_3G_2G_1=
\begin{pmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
0 & -4 & 1
\end{pmatrix}
\begin{pmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
-3 & 0 & 1
\end{pmatrix}
\begin{pmatrix}
1 & 0 & 0\\
-2 & 1 & 0\\
0 & 0 & 1
\end{pmatrix}=
\begin{pmatrix}
1 & 0 & 0\\
-2 & 1 & 0\\
5 & -4 & 1
\end{pmatrix}
$$
从而有$G_3G_2G_1A=U$，即$A=G_{1}^{-1}G_{2}^{-1}G_{3}^{-1}U$。因此$A=LU$，为一个下三角于一个上三角矩阵的乘积，因此称为$LU$分解。
注意：
1. U是高斯消元的结果，且对角线上是主元
2. L对角线上是1，对角线下面的元素image恰恰是在式1中用于消去(i, j)位置上元素的乘子

$LU$分解常用来求解线性方程组，求逆矩阵或者计算行列式。例如在计算行列式的时候，$A=LU,det(A)=det(L)det(U)$。而对于三角矩阵来说，行列式的值即为对角线上元素的乘积。所以如果对矩阵进行三角分解以后再求行列式，会很容易。
在线性代数中已经证明，如果方阵$A$是非奇异的，即$A$的行列式不为0，$LU$分解总是存在的。
## QR分解
QR分解是将矩阵分解为一个正交矩阵与上三角矩阵的乘积。用一张图可以形象地表示QR分解：
![image](http://orddcmy5o.bkt.clouddn.com/rec_sys_QR%E5%88%86%E8%A7%A3.jpg)
这其中，$Q$为正交矩阵，$QQ^T=I$，$R$为上三角矩阵。实际中，QR分解经常被用来解线性最小二乘问题。
# 推荐系统中的MF
1. 属性(properties)
- 发现数据中的潜在结构
- 有一个优雅的概率解释(probabilistic interpretation)
- 容易扩展到一些指定特定先验信息的领域
- 许多优化方法如梯度下降，可以用来寻找最优解
2. 四种类型
- 基本矩阵分解(Basic MF)
- 正则化矩阵分解(Regularized MF)
- 非负矩阵分解(Non-negative MF)
- 基于概率的矩阵分解(PMF)

## Basic MF
Basic MF是最基础的分解方式，将评分矩阵R分解为用户矩阵U和项目矩阵S， 通过不断的迭代训练使得U和S的乘积越来越接近真实矩阵，矩阵分解过程如图：
![image](http://orddcmy5o.bkt.clouddn.com/rec_sys_BMF.jpg)
预测值接近真实值就是使其差最小，这是我们的目标函数，然后采用梯度下降的方式迭代计算U和S，它们收敛时就是分解出来的矩阵。用损失函数来表示误差（等价于目标函数）：
$$
\sum_{i=1}^{n}\sum_{j=1}^{m}(R_{ij}-U_i^TS_j)^2
$$
公式(1)中$R_{ij}$是评分矩阵中已打分的值，$U_{i}$和$S_{j}$未知。为求得公式(1)的最小值，相当于求关于U和S二元函数的最小值（极小值）。通常采用梯度下降法：
$$
U_i\leftarrow U_i-\eta\frac{\partial \mathcal L}{\partial U_i}
\\S_j\leftarrow S_j-\eta\frac{\partial \mathcal L}{\partial S_j}
$$
$\eta$是学习速率，表示迭代步长，其值为1.5时，通常以震荡形式接近极值点；若<1迭代单调趋向极值点；若>2围绕极值逐渐发散，不会收敛到极值点。
## Regularized MF
RMF是BMF的优化，解决MF造成的**过拟合**问题。其不是直接最小化损失函数，而是在损失函数基础上增加规范化因子，将整体作为损失函数。
$$
\min_{U,V}\sum_{(i,j)\in K}(R_{ij}-\sum_{d=1}^{D}U_{di}S_{dj})^2+\lambda(\|U_i\|^2+\|S_j\|^2)
$$
$\lambda$是正则化因子，求解$U$和$S$仍采用梯度下降，此时迭代公式变为：
$$
U_{di}=U_{di}+\alpha(E_{ij}V_{dj}-\lambda U_{di})
\\V_{dj}=V_{dj}+\alpha(E_{ij}U_{di}-\lambda V_{dj})
$$
其中$E_{ij}=R_{ij}-\sum_{d=1}^D{U_{di}V_{dj}}$，梯度下降结束条件为f(x)真实值和预测值小于自己设定的阈值。
### LFM/Funk-SVD
现实中User-Item评分矩阵维度较高且极为稀疏，传统的奇异值分解方法只能对稠密矩阵进行分解，即不允许所分解矩阵有空值。因而，若采用奇异值分解，需要首先填充User-Item评分矩阵，显然，这样造成了两个问题。
- 其一，填充大大增加了数据量，增加了算法复杂度。
- 其二，简单粗暴的数据填充很容易造成数据失真。

这些问题导致了传统的SVD矩阵分解表现并不理想。之后，Simon Funk在博客上公开发表了一个只考虑已有评分记录的矩阵分解方法，称为[Funk-SVD](http://sifter.org/~simon/journal/20061211.html)，也就是被Yehuda Koren称为隐语义模型的矩阵分解方法。他简单地认为，既然我们的评价指标是均方根误差(Root Mean Squared Error, RMSE)，那么可以直接通过训练集中的观察值利用最小化RMSE学习用户特征矩阵P和项目特征Q，并用通过一个正则化项来避免过拟合。其需要优化的函数为:
$$
\min_{p^*,q*}\sum_{(u,i)\in K}(r_{ui}-q_i^Tp_u)^2+\lambda(\|q_i\|^2+\|p_u\|^2)
$$
可以看出该公式与RMF相同，其中$K$为已有评分记录 $(u,i)$ 的集合，$r_{ui}$ 为用户$u$对项目$i$的真实评分，优化方法可以采用交叉最小二乘或随即梯度下降。若输入评分矩阵$R$为M×N矩阵，优化损失函数得到用户特征矩阵$P$ (M×K)和项目特征矩阵$Q$(K×N)，其中$K\ll M,N$，评分预测方法为：
$$
r_{ui}=q_i^{*T}p_u^*
$$
$q_i^{*T}$ 和 $p_u^*$ 是用户$u$和项目$i$的特征向量。
### 带偏置的矩阵分解
基本的矩阵分解方法通过学习用户和项目的特征向量进行预测，即用户和项目的交互信息。用户的特征向量代表了用户的兴趣，项目的特征向量代表了项目的特点，且每一个维度相对应，两个向量的内积表示用户对该项目的喜好程度。但是我们观测到的评分数据大部分都是由与用户或项目无关的因素产生的效果，即有很大一部分因素是和用户对项目的喜好无关而只取决于用户或项目本身特性的。
例如，对于乐观的用户来说，它的评分行为普遍偏高，而对批判性用户来说，他的评分记录普遍偏低，即使他们对同一项目的评分相同，但是他们对该项目的喜好程度却并不一样。同理，对项目来说，以电影为例，受大众欢迎的电影得到的评分普遍偏高，而一些烂片的评分普遍偏低，这些因素都是独立于用户或产品的因素，而和用户对产品的的喜好无关。
我们把这些独立于用户或独立于项目的因素称为**偏置(Bias)部分**，将用户和项目的交互即用户对项目的喜好部分称为**个性化部分**。事实上，在矩阵分解模型中偏好部分对提高评分预测准确率起的作用要大大高于个性化部分所起的作用，以Netflix Prize推荐比赛数据集为例为例，Yehuda Koren仅使用偏置部分可以将评分误差降低32%，而加入个性化部分能降低42%，也就是说只有10%是个性化部分起到的作用，这也充分说明了偏置部分所起的重要性，剩下的58%的误差Yehuda Koren将称之为模型不可解释部分，包括数据噪音等因素。因此在上面的式子中一般都会加入偏置项，$u,b_i,b_u$。综合用下面的式子表示
$$
b_{ui}=\mu+b_i+b_u
$$
偏置部分主要由三个子部分组成，分别是
- 训练集中所有评分记录的全局平均数 $\mu$，表示了训练数据的总体评分情况，对于固定的数据集，它是一个常数。
- 用户偏置$b_u$，独立于项目特征的因素，表示某一特定用户的打分习惯。例如，对于批判性用户对于自己的评分比较苛刻，倾向于打低分；而乐观型用户则打分比较保守，总体打分要偏高。
- 项目偏置$b_i$，独立于用户兴趣的因素，表示某一特定项目得到的打分情况。以电影为例，好片获得的总体评分偏高，而烂片获得的评分普遍偏低，项目偏置捕获的就是这样的特征。

**以上所有偏置和用户对项目的喜好无关**，我们将偏置部分当作基本预测，**在此基础上添加用户对项目的喜好信息，即个性化部分**，因此得到总评分预测公式如下：
$$
\min_{p^*,q*}\sum_{(u,i)\in K}(r_{ui}-\mu-b_u-b_i-q_i^Tp_u)^2+\lambda(\|q_i\|^2+\|p_u\|^2+b_u^2+b_i^2)
$$
## PMF
PMF是在RMF的基础上，引入概率模型进一步优化。假设用户U和项目V的特征矩阵均服从高斯分布，通过评分矩阵已知值得到$U$和$V$的特征矩阵，然后用特征矩阵去预测评分矩阵中的未知值。
若用户$U$的特征矩阵满足均值为0，方差为 $\sigma$的高斯分布，则有如下等式。之所以连乘，是因为$U$的每个观察值$U_i$都是独立同分布的。
$$
p(U|\sigma_U^2)=\prod_{i=1}^NN(U_i|0,\sigma_U^2I)
$$
同理，项目$V$的特征矩阵满足如下等式：
$$
p(U|\sigma_V^2)=\prod_{j=1}^MN(V_j|0,\sigma_V^2I)
$$
其中$N(x|u,\sigma^2)$ 表示变量$x$满足均值为$0$，方差为 $\sigma^2$的高斯分布。
假设真实值$R$和预测值$R^*$ 之差也符合高斯分布，那么有如下概率分布表示，$P(R_{ij}-U_i^TV_j|0,\delta^2)$ 通过平移有 $P(R_{ij}|U_i^TV_j,\delta^2)$，那么评分矩阵$R$的条件概率如下：
$$
P(R_{ij}|U_i^TV_j,\delta^2)=\prod_{i=1}^N\prod_{j=1}^M[N(R_{ij}|U_i^TV_j,\sigma^2)]^{I_{ij}}
$$
这里$U$和$V$是参数，其余看作超参数（即作为$U$和$V$的参数——参数的参数，PMF中通过人工调整超参数，而[BPMF](https://www.cs.toronto.edu/~amnih/papers/bpmf.pdf)通过MCMC方法自动选出最优超参数）。假设$U$和$V$互相独立，可以通过贝叶斯公式得到$R,U,V$的联合分布：
$$\begin{eqnarray}
P(U,V|R,\sigma^2,\sigma_U^2,\sigma_V^2)
&=&P(R|U,V,\sigma^2)P(U|\sigma_U^2)P(V|\sigma_V^2)\\
&=&\prod_{i=1}^N\prod_{j=1}^M[N(R_{ij}|U_i^TV_j,\sigma^2)]^{I_{ij}}\prod_{i=1}^NN(U_i|0,\sigma_U^2I)\prod_{j=1}^MN(V_j|0,\sigma_V^2I)
\end{eqnarray}$$
